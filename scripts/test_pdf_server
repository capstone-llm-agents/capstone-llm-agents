import functools
import logging
import traceback
from collections.abc import Callable
from datetime import datetime
from typing import Any, TypeVar, cast

import uvicorn
from autogen import ConversableAgent
from calendar_functions import convert_ics_to_text, create_ics_file
from mcp.server import Server
from mcp.server.fastmcp import FastMCP
from mcp.server.sse import SseServerTransport
from starlette.applications import Starlette
from starlette.requests import Request
from starlette.responses import Response
from starlette.routing import Mount, Route

from dotenv import load_dotenv

load_dotenv()
from server_llm_config import Model_type, llm_config

pdf_agent = ConversableAgent(
    name="pdf_agent",
    system_message="""Your Job is to assist the user with their tasks.
    """,
    llm_config=llm_config,
    human_input_mode="NEVER",
)
########llm agent setup########

# create the MCP server
mcp = FastMCP("SSE Example Server")


########## Weather agent tools ###############
@mcp.tool(name="create_pdf_file")
def create_pdf_file(prompt):
    try:
        print("Using create pdf function")
        #current_date = datetime.now().date()

        agent_prompt = f"""
        Tasks: {prompt}

        Based off what task you are provided you are to follow the following rules on how you will format your response to be created into a PDF file.
        Any time you create a new line it will also create a new line in the text.
        The first line you write will be the title.
        Always keep the title short and concise.
        Do not add any unesisary text such asl clarifications or thank youse only write what you want to add to the file.
        You may use dot points or paragraphs do what you think will look best.
        """

        extracted_details = pdf_agent.generate_reply(messages=[{"role": "user", "content": agent_prompt}])
        print("#####LLM response#####")
        if Model_type == 1:
            LLM_details = extracted_details["content"]
            print(LLM_details)
        elif Model_type == 2:
            LLM_details = extracted_details
            print(LLM_details)
        else:
            print("Error within weather_server.py response collection")

        result = LLM_details
    except:
        result = "An error has occurred. make sure the date range is no more than 16 days past today. If this is not the issue than it will likely be somewhere in the system"  # temporary error catching

        return result

def create_starlette_app(mcp_server: Server, *, debug: bool = False) -> Starlette:
    """Create a Starlette application that can serve the MCP server with SSE."""
    sse = SseServerTransport("/messages/")

    async def handle_sse(request: Request) -> Response:
        async with sse.connect_sse(
            request.scope,
            request.receive,
            request._send,  # noqa: SLF001 (request._send is a private method, but needed for Starlette's internal use)
        ) as (read_stream, write_stream):
            await mcp_server.run(
                read_stream,
                write_stream,
                mcp_server.create_initialization_options(),
            )

        # 204 - No Content response
        # This is to indicate that the SSE connection is established and no immediate response is needed.
        return Response(status_code=204)

    return Starlette(
        debug=debug,
        routes=[
            Route("/sse", endpoint=handle_sse),
            Mount("/messages/", app=sse.handle_post_message),
        ],
    )


if __name__ == "__main__":
    # get the underlying MCP server so we can give it to Starlette
    mcp_server = mcp._mcp_server  # noqa: SLF001

    # create Starlette app with SSE support
    starlette_app = create_starlette_app(mcp_server, debug=True)

    port = 8082

    # run the server using uvicorn
    uvicorn.run(starlette_app, host="localhost", port=port)
